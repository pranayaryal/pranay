{"name":"Pranay","tagline":"","body":"In the last video, we talked\r\nabout the process of evaluating\r\nan anomaly detection algorithm and\r\nthere we started to use some\r\nlabelled data, with examples\r\nthat we knew were either anomalous\r\nor not anomalous, with y equals 1 or y equals 0.\r\nSo the question then arises, if\r\nwe have this labeled data,\r\nwe have some examples that are\r\nknown to be anomalies and some\r\nthat are known not to be not\r\nanomalies, why don't we\r\njust use a supervised learning algorithm,\r\nso why don't we just use\r\nlogistic regression or a neural\r\nnetwork to try to\r\nlearn directly from our labeled\r\ndata, to predict whether y equals one or y equals zero.\r\nIn this video, I'll try to\r\nshare with you some of\r\nthe thinking and some guidelines for\r\nwhen you should probably use an\r\nanomaly detection algorithm and when\r\nit might be more fruitful to consider\r\nusing a supervised learning algorithm.\r\nThis slide shows, what are\r\nthe settings under which you should\r\nmaybe use anomaly detection versus\r\nwhen supervised learning might be more fruitful.\r\nIf you have a problem with a\r\nvery small number of positive\r\nexamples, and remember examples of\r\ny equals one are the\r\nanomalous examples, then\r\nyou might consider using an anomaly detection algorithm inset.\r\nSo having 0 to 20,\r\nmaybe up to 50 positive examples,\r\nmight be pretty typical, and usually,\r\nwe have such a small set\r\nof positive examples,\r\nwe are going to save the positive\r\nexamples just for the cross\r\nvalidation sets and test sets.\r\nIn contrast, in a typical\r\nnormal anomaly detection setting,\r\nwe will often have a relatively\r\nlarge number of negative examples,\r\nof these normal examples of\r\nnormal aircraft engines.\r\nAnd we can then use this very\r\nlarge number of negative examples,\r\nwith which to fit the model\r\np of x.  And so, there is\r\nthis idea in many anomaly detection\r\napplications, you have\r\nvery few positive examples, and\r\nlots of negative examples, and when\r\nwe are doing the process of\r\nestimating p of x, of fitting all those Gaussian parameters,\r\nwe need only negative examples to do that.\r\nSo if you have a lot of negative data,\r\nwe can still fit to p of x pretty well.\r\nIn contrast, for supervised learning,\r\nmore typically we would have\r\na reasonably large number of\r\nboth positive and negative examples.\r\nAnd so this is one\r\nway to look at your problem\r\nand decide if you should use\r\nan anomaly detection algorithm or a supervised learning algorithm.\r\nHere is another way people often think about anomaly detection algorithms.\r\nSo, for anomaly detection applications\r\noften there are many\r\ndifferent types of anomalies.\r\nSo think about aircraft engines.\r\nYou know there are so many different ways for aircraft engines to go wrong.\r\nRight? There are so many things that could go wrong that could break an aircraft engine.\r\nAnd so, if that's the\r\ncase and you have a pretty small\r\nset of positive examples, then\r\nit can be difficult for\r\nan algorithm to learn from your small\r\nset of positive examples what the anomalies look like.\r\nAnd in particular,\r\nyou know, future anomalies may look\r\nnothing like the ones you've seen so far.\r\nSo maybe in your set\r\nof positive examples, maybe you\r\nhad seen 5 or 10, or 20\r\ndifferent ways that an aircraft engine could go wrong.\r\nBut maybe tomorrow, you\r\nneed to detect a totally\r\nnew set, a totally new\r\ntype of anomaly, a totally\r\nnew way for an aircraft\r\nengine to be broken that\r\nyou have just never seen before,\r\nand if that is the case,\r\nthen it might be more\r\npromising to just model\r\nthe negative examples, with a\r\nsort of a Gaussian model\r\nP of X. Rather than try\r\ntoo hard to model the positive\r\nexamples, because, you know,\r\ntomorrow's anomaly may be\r\nnothing like the ones you've seen so far.\r\nIn contrast, in some other\r\nproblems you have enough\r\npositive examples for an algorithm\r\nto get a sense of what the positive examples are like.\r\nAnd in particular, if you\r\nthink that future positive examples\r\nare likely to be similar\r\nto ones in the training set,\r\nthen in that setting it might\r\nbe more reasonable to have a supervised learning algorithm,\r\nthat looks at a lot of\r\nthe positive examples, looks at a\r\nlot of the negative examples, and\r\nuses that to try to distinguish between positives and negatives.\r\nSo hopefully this gives you\r\na sense of if you have\r\na specific problem you should think\r\nabout using the anomaly\r\ndetection algorithm or a supervised learning algorithm.\r\nAnd the key difference really is,\r\nthat in anomaly detection, after\r\nwe have such a small\r\nnumber of positive examples that there\r\nis not possible, for a learning\r\nalgorithm to learn that much from the positive examples.\r\nAnd so what we do instead,\r\nis take a large set of\r\nnegative examples, and have it just\r\nlearned a lot, learned p\r\nof x from just the negative\r\nexamples of the normal aircraft engines, say.\r\nAnd we reserve the small\r\nnumber of positive examples for evaluating our algorithm\r\nto use in either the cross validation sets or the test sets.\r\nAnd just as a side comment about\r\nthese many different types of\r\nanomalies, you know, in\r\nsome earlier videos we talked\r\nabout the email SPAM examples.\r\nIn those examples, there are\r\nactually many different types of SPAM email.\r\nThe SPAM email is trying to\r\nsell you things spam email, trying to steal your passwords,\r\nthis is called fishing emails, and many different types of SPAM emails.\r\nBut for the SPAM problem, we usually\r\nhave enough examples of spam\r\nemail to see, you know,\r\nmost of these different types of\r\nSPAM email, because we have a\r\nlarge set of examples of\r\nSPAM, and that's why we\r\nusually think of SPAM as\r\na supervised learning setting, even\r\nthough, you know, there may be\r\nmany different types of SPAM.\r\nAnd so, if we look at\r\nsome applications of anomaly detection\r\nversus supervised learning, we'll find\r\nthat, in fraud detection, if\r\nyou have many different types\r\nof ways for people to\r\ntry to commit fraud, and a\r\nrelevantly small training set, a\r\nsmall number of fraudulent users\r\non your website, then I would use an anomaly detection algorithm.\r\nI should say, if you\r\nhave, if you are very a\r\nmajor online retailer, and\r\nif you actually have had a\r\nlot of people try to commit\r\nfraud on your website, so if\r\nyou actually have a lot of\r\nexamples where y equals 1, then\r\nyou know, sometimes fraud detection\r\ncould actually shift over to the supervised learning column.\r\nBut, if you\r\nhaven't seen that many\r\nexamples of users doing\r\nstrange things on your website\r\nthen, more frequently, fraud detection\r\nis actually treated as an\r\nanomaly detection algorithm, rather than one of the supervised learning algorithm.\r\nOther examples, we talked about\r\nmanufacturing already, hopefully you'll\r\nsee more normal examples,\r\nnot that many anomalies.\r\nBut then again, for some manufacturing\r\nprocesses, if you're\r\nmanufacturing very large volumes\r\nand you've seen a lot\r\nof bad examples, maybe manufacturing\r\ncould shift to the supervised learning column as well.\r\nBut, if you haven't seen that\r\nmany bad examples of\r\nthe old products, then I'll do this anomaly detection.\r\nMonitoring machines in the\r\ndata center, again similar\r\nsorts of arguments apply.\r\nWhereas, email SPAM\r\nclassification, weather prediction, and classifying\r\ncancers, if you have\r\nequal numbers of positive and\r\nnegative examples, a lot of you\r\nhave many examples of your\r\npositive and your negative\r\nexamples, then, we would tend to\r\ntreat all of these as supervised learning problems.\r\nSo, hopefully, that gives you\r\na sense of what are the\r\nproperties of a learning\r\nproblem that would cause you to\r\ntreat it as an anomaly\r\ndetention problem verses a supervised learning\r\nproblem.\r\nAnd for many of the problems that are\r\nfaced by various technology companies\r\nand so on, we actually are\r\nin these settings where we have\r\nvery few or sometimes zero\r\npositive training examples,\r\nmaybe there are so many\r\ndifferent types of anomalies that we've never\r\nseen them before, and for those\r\nsorts of problems, very often,\r\nthe algorithm that is used\r\nis an anomaly detection algorithm.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}